{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2d1e9d-8962-4876-aa61-26ea991d7bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Nice, youâ€™ve now got a best model from Keras Tuner ðŸŽ¯\n",
    "From here there are **two big levers**:\n",
    "\n",
    "1. **Make the model itself better** (architecture, regularization, hyperparams)\n",
    "2. **Make the *data & training* setup better** (features, targets, training schedule)\n",
    "\n",
    "Iâ€™ll give you concrete things you can do *right now* with code.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Fine-tune your current best model\n",
    "\n",
    "You already have `best_model` and `best_hp`. Easiest improvement:\n",
    "**keep training the same model with a smaller learning rate.**\n",
    "\n",
    "```python\n",
    "# Take the tuned model\n",
    "best_model = tuner.hypermodel.build(best_hp)\n",
    "\n",
    "# Optionally re-train from scratch with tuned hp\n",
    "history = best_model.fit(\n",
    "    X_train_normal,\n",
    "    y_train,\n",
    "    validation_data=(X_test_normal, y_test),\n",
    "    epochs=100,\n",
    "    callbacks=callbacks,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Now fine-tune with smaller lr\n",
    "import keras\n",
    "\n",
    "old_lr = best_hp.get(\"learning_rate\")\n",
    "fine_tune_lr = old_lr * 0.1\n",
    "\n",
    "best_model.compile(\n",
    "    loss=\"mae\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=fine_tune_lr),\n",
    "    metrics=[\"mae\", \"mse\"]\n",
    ")\n",
    "\n",
    "history_ft = best_model.fit(\n",
    "    X_train_normal,\n",
    "    y_train,\n",
    "    validation_data=(X_test_normal, y_test),\n",
    "    epochs=50,\n",
    "    callbacks=callbacks,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "best_model.evaluate(X_test_normal, y_test)\n",
    "```\n",
    "\n",
    "This alone can squeeze a bit more performance by â€œpolishingâ€ weights.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Add regularization (dropout / L2) and tune it\n",
    "\n",
    "Right now your model is:\n",
    "\n",
    "* Dense â†’ Dense â†’ Dense(1)\n",
    "\n",
    "You can improve **generalization** by:\n",
    "\n",
    "* Adding **L2 weight decay**\n",
    "* Adding a **Dropout** layer\n",
    "\n",
    "And tune their strength.\n",
    "\n",
    "```python\n",
    "import keras\n",
    "import keras_tuner as kt\n",
    "\n",
    "def build_model(hp: kt.HyperParameters):\n",
    "    keras.utils.set_random_seed(42)\n",
    "\n",
    "    units_1 = hp.Choice('units_1', [64, 100, 128])\n",
    "    units_2 = hp.Choice('units_2', [64, 100, 128])\n",
    "    lr      = hp.Choice('learning_rate', [0.001, 0.005, 0.01])\n",
    "\n",
    "    # New: regularization hyperparams\n",
    "    l2_factor = hp.Choice('l2_factor', [0.0, 1e-4, 1e-3])\n",
    "    dropout_rate = hp.Choice('dropout_rate', [0.0, 0.1, 0.2])\n",
    "\n",
    "    reg = keras.regularizers.l2(l2_factor) if l2_factor > 0 else None\n",
    "\n",
    "    model = keras.Sequential(name=\"Model_A\")\n",
    "    model.add(keras.layers.Input(shape=(11,)))\n",
    "    model.add(keras.layers.Dense(units_1, activation='relu',\n",
    "                                 kernel_regularizer=reg))\n",
    "    model.add(keras.layers.Dense(units_2, activation='relu',\n",
    "                                 kernel_regularizer=reg))\n",
    "    # Optional dropout layer\n",
    "    if dropout_rate > 0:\n",
    "        model.add(keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "    model.add(keras.layers.Dense(1))\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"mae\",\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "        metrics=[\"mae\", \"mse\"]\n",
    "    )\n",
    "    return model\n",
    "```\n",
    "\n",
    "Then rerun your `GridSearch` (or better: switch to `RandomSearch` or `BayesianOptimization` so the search space can be larger):\n",
    "\n",
    "```python\n",
    "tuner = kt.RandomSearch(\n",
    "    hypermodel=build_model,\n",
    "    objective=\"val_mae\",\n",
    "    max_trials=30,\n",
    "    overwrite=True,\n",
    "    directory=\"kt_dir\",\n",
    "    project_name=\"model_a_reg\"\n",
    ")\n",
    "\n",
    "tuner.search(\n",
    "    X_train_normal, y_train,\n",
    "    validation_data=(X_test_normal, y_test),\n",
    "    epochs=150,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Tune batch size & number of layers\n",
    "\n",
    "Two more impactful knobs:\n",
    "\n",
    "* **batch_size** (small batches can help generalization sometimes)\n",
    "* **depth** (1, 2, or 3 hidden layers)\n",
    "\n",
    "### Example: tuning batch size\n",
    "\n",
    "```python\n",
    "batch_size = hp.Choice(\"batch_size\", [16, 32, 64, 128])\n",
    "...\n",
    "# in tuner.search:\n",
    "tuner.search(\n",
    "    X_train_normal,\n",
    "    y_train,\n",
    "    validation_data=(X_test_normal, y_test),\n",
    "    epochs=150,\n",
    "    batch_size=best_hp.get(\"batch_size\"),   # if you add it\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "```\n",
    "\n",
    "Better pattern: put `batch_size` also in `search` as:\n",
    "\n",
    "```python\n",
    "tuner.search(\n",
    "    X_train_normal,\n",
    "    y_train,\n",
    "    validation_data=(X_test_normal, y_test),\n",
    "    epochs=150,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    "    batch_size=hp.Choice(\"batch_size\", [32, 64, 128])  # in a custom run_trial\n",
    ")\n",
    "```\n",
    "\n",
    "But the simple way is: try a few batch sizes manually once you have a good model.\n",
    "\n",
    "### Example: tuning number of hidden layers\n",
    "\n",
    "You can optionally add a 3rd dense layer only in some trials:\n",
    "\n",
    "```python\n",
    "def build_model(hp):\n",
    "    keras.utils.set_random_seed(42)\n",
    "\n",
    "    lr = hp.Choice(\"learning_rate\", [0.001, 0.005, 0.01])\n",
    "    num_hidden = hp.Choice(\"num_hidden\", [1, 2, 3])\n",
    "\n",
    "    model = keras.Sequential(name=\"Model_A\")\n",
    "    model.add(keras.layers.Input(shape=(11,)))\n",
    "\n",
    "    units = hp.Choice(\"units\", [64, 100, 128])\n",
    "\n",
    "    for i in range(num_hidden):\n",
    "        model.add(keras.layers.Dense(units, activation=\"relu\"))\n",
    "\n",
    "    model.add(keras.layers.Dense(1))\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"mae\",\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "        metrics=[\"mae\", \"mse\"],\n",
    "    )\n",
    "    return model\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Improve input / target side (often bigger gains)\n",
    "\n",
    "Sometimes the **biggest MAE improvement** comes from data, not layers:\n",
    "\n",
    "1. **Check scaling of targets (`y`)**\n",
    "\n",
    "   * If `y` has a long tail, try a **log transform**: `y_log = np.log1p(y)`\n",
    "   * Train the model on `y_log`, and when predicting, invert with `np.expm1(pred)`.\n",
    "\n",
    "2. **Handle outliers**\n",
    "\n",
    "   * Clip extreme values of target or features.\n",
    "   * Or train with a robust loss (e.g., Huber, which you already played with).\n",
    "\n",
    "3. **Feature engineering**\n",
    "\n",
    "   * Add interactions: `x1 * x2`, ratios, or bucketed versions of continuous features.\n",
    "   * If there are categorical variables, use proper one-hot or embeddings instead of arbitrary numeric codes.\n",
    "\n",
    "4. **Cross-validation**\n",
    "\n",
    "   * Instead of a single train/val split, use **KFold** and average MAE.\n",
    "   * This gives more stable hyperparameter selection and can reveal overfitting to a specific split.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Quick checklist â€“ what Iâ€™d do in your notebook\n",
    "\n",
    "If you want a practical order:\n",
    "\n",
    "1. âœ… **Restart kernel** and use only `keras` + `keras_tuner` (no `tf.keras` models/callbacks).\n",
    "2. âœ… Keep your current best hyperparams and:\n",
    "\n",
    "   * Recompile with `lr * 0.1`\n",
    "   * Fine-tune 30â€“50 more epochs with callbacks.\n",
    "3. âœ… Extend tuner search:\n",
    "\n",
    "   * Add **L2 + Dropout** hyperparams.\n",
    "   * Maybe allow `num_hidden` âˆˆ {1, 2, 3}.\n",
    "   * Switch to `RandomSearch` with ~30 trials.\n",
    "4. âœ… Examine distribution of `y_train`:\n",
    "\n",
    "   * If skewed â†’ try `log1p` target.\n",
    "5. âœ… If MAE still doesnâ€™t move much, weâ€™re probably close to the **information limit of your data** (noise level, missing features, etc.).\n",
    "\n",
    "If you paste your **best_model.evaluate output** and a short description of what the target is (range, approx noise), I can suggest very specific next tweaks.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f90e0777-2ee2-46ec-97e6-d48426440e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n2. Use a smarter tuner than GridSearch\\n\\nGridSearch explodes combinatorially and often wastes trials. For neural nets, RandomSearch or BayesianOptimization usually find better configs faster.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "2. Use a smarter tuner than GridSearch\n",
    "\n",
    "GridSearch explodes combinatorially and often wastes trials. For neural nets, RandomSearch or BayesianOptimization usually find better configs faster.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF+PyTorch (Docker)",
   "language": "python",
   "name": "tf-pytorch-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
